{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backend_bases import RendererBase\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.fftpack import fft\n",
    "%matplotlib inline\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading files containing audio and the path where the image has to be stored \n",
    "audio_path = '..input/kan_audio_2'\n",
    "pict_Path = '..input/picturedata'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subFolderList = []\n",
    "for x in os.listdir(audio_path):\n",
    "    if os.path.isdir(audio_path + '/' + x):\n",
    "        subFolderList.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(pict_Path):\n",
    "    os.makedirs(pict_Path)\n",
    "subFolderList = []\n",
    "for x in os.listdir(audio_path):\n",
    "    if os.path.isdir(audio_path + '/' + x):\n",
    "        subFolderList.append(x)\n",
    "        if not os.path.exists(pict_Path + '/' + x):\n",
    "            os.makedirs(pict_Path +'/'+ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_audio = []\n",
    "for i in subFolderList:\n",
    "    files=librosa.util.find_files(audio_path+\"/\"+i)\n",
    "    files=np.asarray(files)\n",
    "    for j in files:\n",
    "        data=librosa.util.find_files(j)\n",
    "        sample_audio.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting audio to spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, _, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "# for each of the samples\n",
    "for i, filepath in enumerate(sample_audio[:9]):\n",
    "    # Make subplots\n",
    "    plt.subplot(3,3,i+1)\n",
    "    \n",
    "    # pull the labels\n",
    "    label = filepath.split('/')\n",
    "    plt.title(label)\n",
    "    \n",
    "    # create spectogram\n",
    "    samplerate, test_sound  = wavfile.read(filepath)\n",
    "    _, spectrogram = log_specgram(test_sound, samplerate)\n",
    "    \n",
    "    plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2img(wav_path, targetdir='', figsize=(4,4)):\n",
    "    fig = plt.figure(figsize=figsize)    \n",
    "    # use soundfile library to read in the wave files\n",
    "    samplerate, test_sound  = wavfile.read(filepath)\n",
    "    _, spectrogram = log_specgram(test_sound, samplerate)\n",
    "    \n",
    "    ## create output path\n",
    "    output_file = wav_path.split('/')[-1].split('.wav')[0]\n",
    "    output_file = targetdir +'/'+ output_file\n",
    "    #plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
    "    plt.imsave('%s.jpg' % output_file, spectrogram)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the spectro image in one folder\n",
    "for i, x in enumerate(subFolderList[:10]):\n",
    "    print(i, ':', x)\n",
    "    all_files = [y for y in os.listdir(audio_path +'/'+ x) if '.wav' in y]\n",
    "    for file in all_files[:30]:\n",
    "        wav2img(audio_path + x + '/' + file, pict_Path +'/'+ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a .csv containing the images and name of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"..input/image_class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = []\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    img = image.load_img('..input/picturedata'+'/'+train['id'][i]+'.jpg')\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    train_image.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting train & test\n",
    "X = np.array(train_image)\n",
    "y = np.array(train.drop(['id', 'label'],axis=1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining conv2d model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=(5, 5), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_file='..input/test_audio_file'\n",
    "image_file='..input/test_image'\n",
    "wav2img(voice_file,image_file)\n",
    "for i in os.listdir(image_file):\n",
    "    img=Image.open(image_file+\"/\"+i)\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(train.columns[2:])\n",
    "proba = model.predict(img.reshape(1,266,480,3))\n",
    "top3 = np.argsort(proba[0])[:-4:-1]\n",
    "for i in range(3):\n",
    "    print(\"{}\".format(classes[top_3[i]])+\" ({:.3})\".format(proba[0][top_3[i]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
